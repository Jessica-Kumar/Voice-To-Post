import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any

# Load the sentence transformer model
# This will download the model if it's not already cached locally
print("Loading SentenceTransformer model 'all-MiniLM-L6-v2'...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# Determine the dimension of the embeddings generated by the model
embedding_dimension = model.get_sentence_embedding_dimension()

# Initialize a local FAISS IndexFlatL2
# This performs brute-force L2 distance search (exact nearest neighbors)
index = faiss.IndexFlatL2(embedding_dimension)

# A simple list to act as our document store, mapping integer FAISS IDs to text
text_store: List[str] = []

def add_text_to_index(text_list: List[str]) -> None:
    """
    Converts strings to embeddings and stores them in the local FAISS index.
    
    Args:
        text_list (List[str]): A list of strings to be indexed.
    """
    global text_store, index
    
    if not text_list:
        return
        
    # Generate embeddings for the provided list of texts
    embeddings = model.encode(text_list)
    
    # Convert embeddings to a float32 numpy array as required by FAISS
    embeddings = np.array(embeddings).astype('float32')
    
    # Add embeddings to the FAISS index
    index.add(embeddings)
    
    # Append the original text to our text store for later retrieval
    text_store.extend(text_list)

def search_index(query_text: str, top_k: int = 3) -> List[Dict[str, Any]]:
    """
    Embeds a query string, searches the FAISS index, and returns matching texts.
    
    Args:
        query_text (str): The search query.
        top_k (int): The maximum number of results to return.
        
    Returns:
        List[Dict[str, Any]]: A list of dictionaries containing the matched text and its distance.
    """
    if index.ntotal == 0:
        return []
        
    # Generate embedding for the search query
    query_embedding = model.encode([query_text])
    query_embedding = np.array(query_embedding).astype('float32')
    
    # Determine the actual value of k to use 
    # (cannot be greater than the total number of items indexed)
    k = min(top_k, index.ntotal)
    
    # Perform the search on the index
    distances, indices = index.search(query_embedding, k)
    
    results = []
    # indices[0] contains the IDs of the closest matching vectors
    for i in range(k):
        idx = indices[0][i]
        if idx != -1 and idx < len(text_store):
            results.append({
                "text": text_store[idx],
                "distance": float(distances[0][i])
            })
            
    return results
